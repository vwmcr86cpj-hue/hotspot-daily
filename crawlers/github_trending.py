"""
GitHub Trendingæ•°æ®æ”¶é›†
"""
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
import re

class GitHubTrendingCrawler:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
        }
    
    def get_trending(self, language="", since="daily"):
        """
        è·å–GitHub Trending
        Args:
            language: ç¼–ç¨‹è¯­è¨€ï¼Œå¦‚"python", "javascript", "go"
            since: daily, weekly, monthly
        """
        if language:
            url = f"https://github.com/trending/{language}?since={since}"
        else:
            url = f"https://github.com/trending?since={since}"
        
        print(f"[{datetime.now().strftime('%H:%M:%S')}] è·å–GitHub Trending ({language or 'all'}/{since})...")
        
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                repos = []
                
                for article in soup.select('article.Box-row'):
                    repo_info = self._parse_repo_element(article)
                    if repo_info:
                        repo_info["language"] = language or "all"
                        repo_info["period"] = since
                        repos.append(repo_info)
                
                print(f"âœ… è·å–åˆ° {len(repos)} ä¸ªçƒ­é—¨ä»“åº“")
                return repos
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ è·å–å¤±è´¥: {e}")
        
        return []
    
    def _parse_repo_element(self, article):
        """è§£æå•ä¸ªä»“åº“å…ƒç´ """
        try:
            # ä»“åº“æ ‡é¢˜å’Œé“¾æ¥
            title_elem = article.select_one('h2 a')
            if not title_elem:
                return None
                
            title = title_elem.get_text(strip=True)
            repo_url = f"https://github.com{title_elem.get('href', '')}"
            
            # æå–ä½œè€…å’Œä»“åº“å
            author, repo_name = "", ""
            if "/" in title:
                parts = title.split("/")
                if len(parts) >= 2:
                    author = parts[0].strip()
                    repo_name = parts[1].strip()
            
            # æè¿°
            desc_elem = article.select_one('p')
            description = desc_elem.get_text(strip=True) if desc_elem else ""
            
            # ç¼–ç¨‹è¯­è¨€
            lang_elem = article.select_one('span[itemprop="programmingLanguage"]')
            language = lang_elem.get_text(strip=True) if lang_elem else "Unknown"
            
            # æ˜Ÿæ ‡æ•°
            stars_text = "0"
            stars_elem = article.select('a[href$="/stargazers"]')
            if stars_elem:
                stars_text = stars_elem[0].get_text(strip=True)
            
            # forksæ•°
            forks_text = "0"
            forks_elem = article.select('a[href$="/forks"]')
            if forks_elem:
                forks_text = forks_elem[0].get_text(strip=True)
            
            # ä»Šæ—¥æ˜Ÿæ ‡å¢é•¿
            stars_today_text = ""
            stars_today_elem = article.select('span.d-inline-block.float-sm-right')
            if stars_today_elem:
                stars_today_text = stars_today_elem[0].get_text(strip=True)
                # æå–æ•°å­—
                match = re.search(r'(\d+[,]?\d*)', stars_today_text)
                if match:
                    stars_today_text = match.group(1).replace(',', '')
            
            return {
                "title": title,
                "author": author,
                "repo_name": repo_name,
                "url": repo_url,
                "description": description,
                "language": language,
                "stars": self._parse_number(stars_text),
                "forks": self._parse_number(forks_text),
                "stars_today": stars_today_text,
                "platform": "GitHub"
            }
            
        except Exception as e:
            print(f"è§£æä»“åº“å¤±è´¥: {e}")
            return None
    
    def _parse_number(self, text):
        """è§£ææ•°å­—æ–‡æœ¬ï¼Œå¦‚1.2k -> 1200"""
        if not text:
            return 0
        
        text = text.replace(',', '').strip()
        
        if 'k' in text.lower():
            try:
                return int(float(text.lower().replace('k', '')) * 1000)
            except:
                return 0
        else:
            try:
                return int(text)
            except:
                return 0
    
    def get_multiple_languages(self, languages=None, since="daily"):
        """è·å–å¤šä¸ªç¼–ç¨‹è¯­è¨€çš„Trending"""
        if languages is None:
            languages = ["", "python", "javascript", "java", "go", "rust"]
        
        all_repos = []
        
        for lang in languages:
            repos = self.get_trending(language=lang, since=since)
            all_repos.extend(repos)
            if lang != languages[-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ª
                time.sleep(2)  # é¿å…è¯·æ±‚å¤ªå¿«
        
        return all_repos
    
    def save_to_file(self, data, filename_prefix="github_trending"):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        if not data:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"data/{filename_prefix}_{timestamp}.json"
        
        os.makedirs("data", exist_ok=True)
        
        save_data = {
            "platform": "github",
            "timestamp": datetime.now().isoformat(),
            "period": data[0].get("period", "daily") if data else "daily",
            "data": data
        }
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        return filename

def test_github_trending():
    """æµ‹è¯•GitHub Trendingçˆ¬è™«"""
    print("=" * 60)
    print("GitHub Trendingçˆ¬è™«æµ‹è¯•")
    print("=" * 60)
    
    crawler = GitHubTrendingCrawler()
    
    # 1. æµ‹è¯•å…¨ç«™çƒ­é—¨
    print("\n1. æµ‹è¯•å…¨ç«™çƒ­é—¨ä»“åº“:")
    repos = crawler.get_trending(since="daily")
    
    if repos:
        print(f"\nğŸ† GitHub Trending TOP5:")
        for i, repo in enumerate(repos[:5], 1):
            print(f"{i:2d}. {repo['title'][:40]:40}")
            print(f"     {repo['description'][:50]:50}")
            print(f"     è¯­è¨€: {repo['language']:10} æ˜Ÿæ ‡: {repo['stars']:,} ä»Šæ—¥: +{repo['stars_today']}")
    
    time.sleep(2)
    
    # 2. æµ‹è¯•Pythonè¯­è¨€
    print("\n2. æµ‹è¯•Pythonè¯­è¨€çƒ­é—¨:")
    python_repos = crawler.get_trending(language="python", since="daily")
    
    if python_repos:
        print(f"\nğŸ Pythonçƒ­é—¨TOP3:")
        for i, repo in enumerate(python_repos[:3], 1):
            print(f"{i:2d}. {repo['title'][:30]}")
    
    # 3. ä¿å­˜æ•°æ®
    if repos:
        crawler.save_to_file(repos, "github_trending_daily")
        
        print("\n" + "=" * 60)
        print("ğŸ¯ GitHub Trendingçˆ¬è™«æµ‹è¯•å®Œæˆï¼")
        print(f"   è·å–ä»“åº“: {len(repos)} ä¸ª")
        if python_repos:
            print(f"   Pythonä»“åº“: {len(python_repos)} ä¸ª")
    else:
        print("\nâŒ GitHub Trendingçˆ¬è™«æµ‹è¯•å¤±è´¥")
    
    return repos

if __name__ == "__main__":
    import os
    test_github_trending()
